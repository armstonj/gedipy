{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the GEDI HDF5 file structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**author**: Stefanie Lumnitz, stefanie.lumnitz@esa.int\n",
    "\n",
    "**goal**: \n",
    "1. discover how to work with HDF5 file format\n",
    "2. explore best practices for HDF5 files\n",
    "3. explore GEDI 2A and 2B data structures & content\n",
    "\n",
    "**content**:\n",
    "* The HDF5 file structure\n",
    "* Performance & Scalability\n",
    "    * HDF5 Best Practices\n",
    "    * Scalability through the HDF5 API and functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDF file structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GEDI data is distributed in `.h5` data format, a Hierarchical Data Format (HDF). It is basically a container holding multidimensional arrays and attributes of scientific data. Imagine a whole unix file structure with directories and values stored in a file. HDF5 can commonly be opened and analysed using one of the following:\n",
    "* [h5py](https://www.h5py.org/)\n",
    "    * can store and work with scalars and numpy arrays\n",
    "    * more pythonic easier to use\n",
    "* PyTables\n",
    "    * stores python objects and classes as attributes\n",
    "    * much faster for reading slices of datasets (as it does not read the array first like h5py)\n",
    "    * supported natively for compression and chuncking of dataset during writing operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on HDF5 check out Tom Kooij's tutorial [HDF5 take 2](https://www.youtube.com/watch?v=ofLFhQ9yxCw)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BEAM0000',\n",
       " 'BEAM0001',\n",
       " 'BEAM0010',\n",
       " 'BEAM0011',\n",
       " 'BEAM0101',\n",
       " 'BEAM0110',\n",
       " 'BEAM1000',\n",
       " 'BEAM1011',\n",
       " 'METADATA']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path_1B = os.path.join(\"/home\",\"stef\",\"Testbed\",\"00_data\",\"GEDI\", \"GEDI01_B_2019122150008_O02186_T04733_02_003_01.h5\")\n",
    "\n",
    "with h5py.File(file_path_1B, 'r') as f_1B: # open file in read mode\n",
    "    keys = list(f_1B)\n",
    "    \n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BEAM0000',\n",
       " 'BEAM0001',\n",
       " 'BEAM0010',\n",
       " 'BEAM0011',\n",
       " 'BEAM0101',\n",
       " 'BEAM0110',\n",
       " 'BEAM1000',\n",
       " 'BEAM1011',\n",
       " 'METADATA']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path_2B = \"/home/stef/Testbed/00_data/GEDI/GEDI02_B_2019113083317_O02042_T04038_02_001_01.h5\"\n",
    "f_2B = h5py.File(file_path_2B, 'r')\n",
    "list(f_2B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method File.close of <HDF5 file \"GEDI02_B_2019113083317_O02042_T04038_02_001_01.h5\" (mode r)>>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_2B.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !h5ls {file_path_2B}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hierarchical HDF5 format is build using *groups*, folder-like containers that hold datasets and other groups, and, *datasets*, array-like collections of data. It resembles a unix file path. GEDI data is organized in 8 groups representing ground-beams (BEAM0000', 'BEAM0001', 'BEAM0010', 'BEAM0011', ..) and Metadata. A description of groups and datasets can also be found [here for level 1 B Products](file:///tmp/mozilla_stef0/gedi_l1b_product_data_dictionary_P003_v1.html) and [here for Level 2B Products](file:///tmp/mozilla_stef0/gedi_l2b_dictionary_P001_v1.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate the BEAM0000 group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list groups (aka dictionaty keys) and attached objects\n",
    "# list(f_2B['BEAM0000'].items())\n",
    "\n",
    "# list (groups) dictionary keys only\n",
    "# list(f_2B['BEAM0000'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printname(name):\n",
    "    \"\"\" prints names of GEDI file\n",
    "    \n",
    "    Note: callback used in combination with f.visit(printname)\n",
    "    \"\"\"\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithmrun_flag\n",
      "ancillary\n",
      "ancillary/dz\n",
      "ancillary/l2a_alg_count\n",
      "ancillary/maxheight_cuttoff\n",
      "ancillary/phony_dim_52\n",
      "ancillary/rg_eg_constraint_center_buffer\n",
      "ancillary/rg_eg_mpfit_max_func_evals\n",
      "ancillary/rg_eg_mpfit_maxiters\n",
      "ancillary/rg_eg_mpfit_tolerance\n",
      "ancillary/signal_search_buff\n",
      "ancillary/tx_noise_stddev_multiplier\n",
      "beam\n",
      "channel\n",
      "cover\n",
      "cover_z\n",
      "fhd_normal\n",
      "geolocation\n",
      "geolocation/degrade_flag\n",
      "geolocation/delta_time\n",
      "geolocation/digital_elevation_model\n",
      "geolocation/elev_highestreturn\n",
      "geolocation/elev_lowestmode\n",
      "geolocation/elevation_bin0\n",
      "geolocation/elevation_bin0_error\n",
      "geolocation/elevation_lastbin\n",
      "geolocation/elevation_lastbin_error\n",
      "geolocation/height_bin0\n",
      "geolocation/height_lastbin\n",
      "geolocation/lat_highestreturn\n",
      "geolocation/lat_lowestmode\n",
      "geolocation/latitude_bin0\n",
      "geolocation/latitude_bin0_error\n",
      "geolocation/latitude_lastbin\n",
      "geolocation/latitude_lastbin_error\n",
      "geolocation/local_beam_azimuth\n",
      "geolocation/local_beam_elevation\n",
      "geolocation/lon_highestreturn\n",
      "geolocation/lon_lowestmode\n",
      "geolocation/longitude_bin0\n",
      "geolocation/longitude_bin0_error\n",
      "geolocation/longitude_lastbin\n",
      "geolocation/longitude_lastbin_error\n",
      "geolocation/phony_dim_53\n",
      "geolocation/shot_number\n",
      "geolocation/solar_azimuth\n",
      "geolocation/solar_elevation\n",
      "l2a_quality_flag\n",
      "l2b_quality_flag\n",
      "land_cover_data\n",
      "land_cover_data/landsat_treecover\n",
      "land_cover_data/modis_nonvegetated\n",
      "land_cover_data/modis_nonvegetated_sd\n",
      "land_cover_data/modis_treecover\n",
      "land_cover_data/modis_treecover_sd\n",
      "land_cover_data/phony_dim_54\n",
      "master_frac\n",
      "master_int\n",
      "num_detectedmodes\n",
      "omega\n",
      "pai\n",
      "pai_z\n",
      "pavd_z\n",
      "pgap_theta\n",
      "pgap_theta_error\n",
      "pgap_theta_z\n",
      "phony_dim_0\n",
      "phony_dim_1\n",
      "phony_dim_2\n",
      "rg\n",
      "rh100\n",
      "rhog\n",
      "rhog_error\n",
      "rhov\n",
      "rhov_error\n",
      "rossg\n",
      "rv\n",
      "rx_processing\n",
      "rx_processing/algorithmrun_flag_a1\n",
      "rx_processing/algorithmrun_flag_a2\n",
      "rx_processing/algorithmrun_flag_a3\n",
      "rx_processing/algorithmrun_flag_a4\n",
      "rx_processing/algorithmrun_flag_a5\n",
      "rx_processing/algorithmrun_flag_a6\n",
      "rx_processing/pgap_theta_a1\n",
      "rx_processing/pgap_theta_a2\n",
      "rx_processing/pgap_theta_a3\n",
      "rx_processing/pgap_theta_a4\n",
      "rx_processing/pgap_theta_a5\n",
      "rx_processing/pgap_theta_a6\n",
      "rx_processing/pgap_theta_error_a1\n",
      "rx_processing/pgap_theta_error_a2\n",
      "rx_processing/pgap_theta_error_a3\n",
      "rx_processing/pgap_theta_error_a4\n",
      "rx_processing/pgap_theta_error_a5\n",
      "rx_processing/pgap_theta_error_a6\n",
      "rx_processing/phony_dim_55\n",
      "rx_processing/rg_a1\n",
      "rx_processing/rg_a2\n",
      "rx_processing/rg_a3\n",
      "rx_processing/rg_a4\n",
      "rx_processing/rg_a5\n",
      "rx_processing/rg_a6\n",
      "rx_processing/rg_eg_amplitude_a1\n",
      "rx_processing/rg_eg_amplitude_a2\n",
      "rx_processing/rg_eg_amplitude_a3\n",
      "rx_processing/rg_eg_amplitude_a4\n",
      "rx_processing/rg_eg_amplitude_a5\n",
      "rx_processing/rg_eg_amplitude_a6\n",
      "rx_processing/rg_eg_amplitude_error_a1\n",
      "rx_processing/rg_eg_amplitude_error_a2\n",
      "rx_processing/rg_eg_amplitude_error_a3\n",
      "rx_processing/rg_eg_amplitude_error_a4\n",
      "rx_processing/rg_eg_amplitude_error_a5\n",
      "rx_processing/rg_eg_amplitude_error_a6\n",
      "rx_processing/rg_eg_center_a1\n",
      "rx_processing/rg_eg_center_a2\n",
      "rx_processing/rg_eg_center_a3\n",
      "rx_processing/rg_eg_center_a4\n",
      "rx_processing/rg_eg_center_a5\n",
      "rx_processing/rg_eg_center_a6\n",
      "rx_processing/rg_eg_center_error_a1\n",
      "rx_processing/rg_eg_center_error_a2\n",
      "rx_processing/rg_eg_center_error_a3\n",
      "rx_processing/rg_eg_center_error_a4\n",
      "rx_processing/rg_eg_center_error_a5\n",
      "rx_processing/rg_eg_center_error_a6\n",
      "rx_processing/rg_eg_chisq_a1\n",
      "rx_processing/rg_eg_chisq_a2\n",
      "rx_processing/rg_eg_chisq_a3\n",
      "rx_processing/rg_eg_chisq_a4\n",
      "rx_processing/rg_eg_chisq_a5\n",
      "rx_processing/rg_eg_chisq_a6\n",
      "rx_processing/rg_eg_flag_a1\n",
      "rx_processing/rg_eg_flag_a2\n",
      "rx_processing/rg_eg_flag_a3\n",
      "rx_processing/rg_eg_flag_a4\n",
      "rx_processing/rg_eg_flag_a5\n",
      "rx_processing/rg_eg_flag_a6\n",
      "rx_processing/rg_eg_gamma_a1\n",
      "rx_processing/rg_eg_gamma_a2\n",
      "rx_processing/rg_eg_gamma_a3\n",
      "rx_processing/rg_eg_gamma_a4\n",
      "rx_processing/rg_eg_gamma_a5\n",
      "rx_processing/rg_eg_gamma_a6\n",
      "rx_processing/rg_eg_gamma_error_a1\n",
      "rx_processing/rg_eg_gamma_error_a2\n",
      "rx_processing/rg_eg_gamma_error_a3\n",
      "rx_processing/rg_eg_gamma_error_a4\n",
      "rx_processing/rg_eg_gamma_error_a5\n",
      "rx_processing/rg_eg_gamma_error_a6\n",
      "rx_processing/rg_eg_niter_a1\n",
      "rx_processing/rg_eg_niter_a2\n",
      "rx_processing/rg_eg_niter_a3\n",
      "rx_processing/rg_eg_niter_a4\n",
      "rx_processing/rg_eg_niter_a5\n",
      "rx_processing/rg_eg_niter_a6\n",
      "rx_processing/rg_eg_sigma_a1\n",
      "rx_processing/rg_eg_sigma_a2\n",
      "rx_processing/rg_eg_sigma_a3\n",
      "rx_processing/rg_eg_sigma_a4\n",
      "rx_processing/rg_eg_sigma_a5\n",
      "rx_processing/rg_eg_sigma_a6\n",
      "rx_processing/rg_eg_sigma_error_a1\n",
      "rx_processing/rg_eg_sigma_error_a2\n",
      "rx_processing/rg_eg_sigma_error_a3\n",
      "rx_processing/rg_eg_sigma_error_a4\n",
      "rx_processing/rg_eg_sigma_error_a5\n",
      "rx_processing/rg_eg_sigma_error_a6\n",
      "rx_processing/rg_error_a1\n",
      "rx_processing/rg_error_a2\n",
      "rx_processing/rg_error_a3\n",
      "rx_processing/rg_error_a4\n",
      "rx_processing/rg_error_a5\n",
      "rx_processing/rg_error_a6\n",
      "rx_processing/rv_a1\n",
      "rx_processing/rv_a2\n",
      "rx_processing/rv_a3\n",
      "rx_processing/rv_a4\n",
      "rx_processing/rv_a5\n",
      "rx_processing/rv_a6\n",
      "rx_processing/rx_energy_a1\n",
      "rx_processing/rx_energy_a2\n",
      "rx_processing/rx_energy_a3\n",
      "rx_processing/rx_energy_a4\n",
      "rx_processing/rx_energy_a5\n",
      "rx_processing/rx_energy_a6\n",
      "rx_processing/shot_number\n",
      "rx_range_highestreturn\n",
      "rx_sample_count\n",
      "rx_sample_start_index\n",
      "selected_l2a_algorithm\n",
      "selected_rg_algorithm\n",
      "sensitivity\n",
      "shot_number\n",
      "stale_return_flag\n",
      "surface_flag\n"
     ]
    }
   ],
   "source": [
    " f_2B['BEAM0000'].visit(printname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a list of all objects in the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_2B_obj = []\n",
    "f_2B.visit(f_2B_obj.append)\n",
    "# f_2B_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the `f_2B['BEAM0000']` group consists on several subgroups. What separates the HDF5 file system from a normal unix path is that attributes, little pieces of metadata can be attached to groups, subgroups and datasets directly. Here is an example, investigating the attributes attached ot the `cover` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened \"/home/stef/Testbed/00_data/GEDI/GEDI02_B_2019113083317_O02042_T04038_02_001_01.h5\" with sec2 driver.\n",
      "BEAM0001/cover           Dataset {343295/343295}\n",
      "    Attribute: DIMENSION_LIST {1}\n",
      "        Type:      variable length of\n",
      "                   object reference\n",
      "        Data:  (DATASET-1:1286593865)\n",
      "    Attribute: _FillValue scalar\n",
      "        Type:      native double\n",
      "        Data:  -9999\n",
      "    Attribute: coordinates scalar\n",
      "        Type:      variable-length null-terminated UTF-8 string\n",
      "        Data:  \"geolocation/delta_time geolocation/lat_lowestmode geolocation/lon_lowestmode\"\n",
      "    Attribute: description scalar\n",
      "        Type:      variable-length null-terminated UTF-8 string\n",
      "        Data:  \"Total canopy cover, defined as the percent of the ground covered by the vertical projection of canopy material\"\n",
      "    Attribute: long_name scalar\n",
      "        Type:      variable-length null-terminated UTF-8 string\n",
      "        Data:  \"Total cover\"\n",
      "    Attribute: units scalar\n",
      "        Type:      variable-length null-terminated UTF-8 string\n",
      "        Data:  \"-\"\n",
      "    Attribute: valid_range {2}\n",
      "        Type:      native float\n",
      "        Data:  0, 1\n",
      "    Location:  1:138938647\n",
      "    Links:     1\n",
      "    Chunks:    {14200} 56800 bytes\n",
      "    Storage:   1373180 logical bytes, 687889 allocated bytes, 199.62% utilization\n",
      "    Filter-0:  deflate-1 OPT {4}\n",
      "    Type:      native float\n"
     ]
    }
   ],
   "source": [
    "!h5ls -vlr {file_path_2B}/BEAM0001/cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = f_2B['BEAM0000/cover']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DIMENSION_LIST',\n",
       " '_FillValue',\n",
       " 'coordinates',\n",
       " 'description',\n",
       " 'long_name',\n",
       " 'units',\n",
       " 'valid_range']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dset.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total canopy cover, defined as the percent of the ground covered by the vertical projection of canopy material'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset.attrs['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'geolocation/delta_time geolocation/lat_lowestmode geolocation/lon_lowestmode'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset.attrs['coordinates']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `coordinates` attribute for example provides the path name or dictionary key to the spatial coordinates used with the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PERFORMANCE & SCALABILITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance through Best Practices:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* access subgroups: `f[../..]` - efficient, `f[][]` - inefficient\n",
    "* use standard Python containership tests: `if 'name' in group`, NOT `if 'name' in group.keys()`\n",
    "* don't forget to close the hd5 file\n",
    "* or open through `with h5py.File(file_path_1B, 'r') as f_1B:` statement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalability through the HDF5 API & Functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**core driver**: the coredriver stores your file entirely in memory, which makes I/O operations incredibly fast. Beware there is a limit to how much data fits into memory, so only use this for quick data exploration of small files. YOU can set the driver to core using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = h5py.File(fiel_path_1B, driver =\"core\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**chunking**: It is possible to chunck hdf5 files and work with chucked files. This is worth doing in case the file is reused many times. Can can speed up I/O operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened \"/home/stef/Testbed/00_data/GEDI/GEDI02_B_2019113083317_O02042_T04038_02_001_01.h5\" with sec2 driver.\n",
      "BEAM0001/cover           Dataset {343295/343295}\n",
      "    Attribute: DIMENSION_LIST {1}\n",
      "        Type:      variable length of\n",
      "                   object reference\n",
      "        Data:  (DATASET-1:1286593865)\n",
      "    Attribute: _FillValue scalar\n",
      "        Type:      native double\n",
      "        Data:  -9999\n",
      "    Attribute: coordinates scalar\n",
      "        Type:      variable-length null-terminated UTF-8 string\n",
      "        Data:  \"geolocation/delta_time geolocation/lat_lowestmode geolocation/lon_lowestmode\"\n",
      "    Attribute: description scalar\n",
      "        Type:      variable-length null-terminated UTF-8 string\n",
      "        Data:  \"Total canopy cover, defined as the percent of the ground covered by the vertical projection of canopy material\"\n",
      "    Attribute: long_name scalar\n",
      "        Type:      variable-length null-terminated UTF-8 string\n",
      "        Data:  \"Total cover\"\n",
      "    Attribute: units scalar\n",
      "        Type:      variable-length null-terminated UTF-8 string\n",
      "        Data:  \"-\"\n",
      "    Attribute: valid_range {2}\n",
      "        Type:      native float\n",
      "        Data:  0, 1\n",
      "    Location:  1:138938647\n",
      "    Links:     1\n",
      "    Chunks:    {14200} 56800 bytes\n",
      "    Storage:   1373180 logical bytes, 687889 allocated bytes, 199.62% utilization\n",
      "    Filter-0:  deflate-1 OPT {4}\n",
      "    Type:      native float\n"
     ]
    }
   ],
   "source": [
    "!h5ls -vlr {file_path_2B}/BEAM0001/cover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see arrays are chunked. See line ```Chunks:    {14200} 56800 bytes```, indicating that there is not much to optimize here.\n",
    "\n",
    "(Other questions of consideration: Do chunks fit data access patterns? Are chunks optimized for spatial subsetting? Because we are doing the I/O operations on a 2 CPU, 4GB RAM core, we need to be beware of the chunk cache. The fact that the kernel is not crashing when opening the file indicated strong performant chunking.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**compression**: In hdf5 uncompressed datasets are faster to read and write. the `.zarr` datafromat for example is highly optimized for BLOSC compression and faster read or written when compressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**reading from disk**: Note here using pd.HDFStore.select and a where query allows you to read file content direclty from disc instead of using memory. This process is slower but allows you to read larger datasets when memory is the bottelneck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**parallel hdf5**: (Note if I/O operations are the bottelneck you will not get any speed up through using parallel processign here, but you will improve CPU intensive processing.) In order to make use of parallel processing with hdf5 check if Threadsafety is no and Parallel HDf5 is yes in your local configuration. You can check your configuration using: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t    SUMMARY OF THE HDF5 CONFIGURATION\n",
      "\t    =================================\n",
      "\n",
      "General Information:\n",
      "-------------------\n",
      "                   HDF5 Version: 1.10.4\n",
      "                  Configured on: Wed Dec 19 18:26:52 UTC 2018\n",
      "                  Configured by: root@3dad7c19-81ba-4672-4f33-547177f88490\n",
      "                    Host system: x86_64-conda_cos6-linux-gnu\n",
      "              Uname information: Linux 3dad7c19-81ba-4672-4f33-547177f88490 4.4.0-62-generic #83-Ubuntu SMP Wed Jan 18 14:10:15 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\n",
      "                       Byte sex: little-endian\n",
      "             Installation point: /home/stef/miniconda3/envs/gedi\n",
      "\n",
      "Compiling Options:\n",
      "------------------\n",
      "                     Build Mode: production\n",
      "              Debugging Symbols: no\n",
      "                        Asserts: no\n",
      "                      Profiling: no\n",
      "             Optimization Level: high\n",
      "\n",
      "Linking Options:\n",
      "----------------\n",
      "                      Libraries: static, shared\n",
      "  Statically Linked Executables: \n",
      "                        LDFLAGS: -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,-rpath,/home/stef/miniconda3/envs/gedi/lib -L/home/stef/miniconda3/envs/gedi/lib\n",
      "                     H5_LDFLAGS: \n",
      "                     AM_LDFLAGS:  -L/home/stef/miniconda3/envs/gedi/lib\n",
      "                Extra libraries: -lrt -lpthread -lz -ldl -lm \n",
      "                       Archiver: /tmp/build/80754af9/hdf5_1545243905949/_build_env/bin/x86_64-conda_cos6-linux-gnu-ar\n",
      "                       AR_FLAGS: cr\n",
      "                         Ranlib: /tmp/build/80754af9/hdf5_1545243905949/_build_env/bin/x86_64-conda_cos6-linux-gnu-ranlib\n",
      "\n",
      "Languages:\n",
      "----------\n",
      "                              C: yes\n",
      "                     C Compiler: /tmp/build/80754af9/hdf5_1545243905949/_build_env/bin/x86_64-conda_cos6-linux-gnu-cc\n",
      "                       CPPFLAGS: -DNDEBUG -D_FORTIFY_SOURCE=2 -O2\n",
      "                    H5_CPPFLAGS: -D_GNU_SOURCE -D_POSIX_C_SOURCE=200112L   -DNDEBUG -UH5_DEBUG_API\n",
      "                    AM_CPPFLAGS:  -I/home/stef/miniconda3/envs/gedi/include\n",
      "                        C Flags: -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe -I/home/stef/miniconda3/envs/gedi/include -fdebug-prefix-map=${SRC_DIR}=/usr/local/src/conda/${PKG_NAME}-${PKG_VERSION} -fdebug-prefix-map=${PREFIX}=/usr/local/src/conda-prefix\n",
      "                     H5 C Flags:  -std=c99  -pedantic -Wall -Wextra -Wbad-function-cast -Wc++-compat -Wcast-align -Wcast-qual -Wconversion -Wdeclaration-after-statement -Wdisabled-optimization -Wfloat-equal -Wformat=2 -Winit-self -Winvalid-pch -Wmissing-declarations -Wmissing-include-dirs -Wmissing-prototypes -Wnested-externs -Wold-style-definition -Wpacked -Wpointer-arith -Wredundant-decls -Wshadow -Wstrict-prototypes -Wswitch-default -Wswitch-enum -Wundef -Wunused-macros -Wunsafe-loop-optimizations -Wwrite-strings -finline-functions -s -Wno-inline -Wno-aggregate-return -Wno-missing-format-attribute -Wno-missing-noreturn -O\n",
      "                     AM C Flags: \n",
      "               Shared C Library: yes\n",
      "               Static C Library: yes\n",
      "\n",
      "\n",
      "                        Fortran: yes\n",
      "               Fortran Compiler: /tmp/build/80754af9/hdf5_1545243905949/_build_env/bin/x86_64-conda_cos6-linux-gnu-gfortran\n",
      "                  Fortran Flags: \n",
      "               H5 Fortran Flags:  -pedantic -Wall -Wextra -Wunderflow -Wimplicit-interface -Wsurprising -Wno-c-binding-type  -s -O2\n",
      "               AM Fortran Flags: \n",
      "         Shared Fortran Library: yes\n",
      "         Static Fortran Library: yes\n",
      "\n",
      "                            C++: yes\n",
      "                   C++ Compiler: /tmp/build/80754af9/hdf5_1545243905949/_build_env/bin/x86_64-conda_cos6-linux-gnu-c++\n",
      "                      C++ Flags: -fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe -I/home/stef/miniconda3/envs/gedi/include -fdebug-prefix-map=${SRC_DIR}=/usr/local/src/conda/${PKG_NAME}-${PKG_VERSION} -fdebug-prefix-map=${PREFIX}=/usr/local/src/conda-prefix\n",
      "                   H5 C++ Flags:   -pedantic -Wall -W -Wundef -Wshadow -Wpointer-arith -Wcast-qual -Wcast-align -Wwrite-strings -Wconversion -Wredundant-decls -Winline -Wsign-promo -Woverloaded-virtual -Wold-style-cast -Weffc++ -Wreorder -Wnon-virtual-dtor -Wctor-dtor-privacy -Wabi -finline-functions -s -O\n",
      "                   AM C++ Flags: \n",
      "             Shared C++ Library: yes\n",
      "             Static C++ Library: yes\n",
      "\n",
      "                           Java: no\n",
      "\n",
      "\n",
      "Features:\n",
      "---------\n",
      "                   Parallel HDF5: no\n",
      "Parallel Filtered Dataset Writes: no\n",
      "              Large Parallel I/O: no\n",
      "              High-level library: yes\n",
      "                    Threadsafety: yes\n",
      "             Default API mapping: v110\n",
      "  With deprecated public symbols: yes\n",
      "          I/O filters (external): deflate(zlib)\n",
      "                             MPE: no\n",
      "                      Direct VFD: no\n",
      "                         dmalloc: no\n",
      "  Packages w/ extra debug output: none\n",
      "                     API tracing: no\n",
      "            Using memory checker: yes\n",
      " Memory allocation sanity checks: no\n",
      "             Metadata trace file: no\n",
      "          Function stack tracing: no\n",
      "       Strict file format checks: no\n",
      "    Optimization instrumentation: no\n"
     ]
    }
   ],
   "source": [
    "!h5cc -showconfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information on how to change your local settings can be found in Tom Kooij's tutorial [HDF5 take 2](https://www.youtube.com/watch?v=ofLFhQ9yxCw) presented at SciPy 2017, minute 2:35:00 onwards. All of the above is interesting for reoccuring and very large data processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
